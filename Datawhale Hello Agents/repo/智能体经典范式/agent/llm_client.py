# å¢åŠ æ¨¡å‹é€‰æ‹©åŠŸèƒ½ï¼Œé»˜è®¤ä½¿ç”¨ Mimo-V2-flash
# ä¸ºæµå¼å“åº”å¢åŠ äº†å®‰å…¨æ£€æŸ¥é€»è¾‘ï¼Œé¿å… `choices` ä¸ºç©ºå¯¼è‡´çš„ç©ºå“åº”ï¼›å¢åŠ éæµå¼å“åº”æ”¯æŒï¼Œé»˜è®¤å…³é—­

import os
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict, Optional

# åŠ è½½ .env æ–‡ä»¶ (ç¡®ä¿èƒ½è¯»å–åˆ°æ ¹ç›®å½•çš„ .env)
# å‡è®¾å½“å‰è¿è¡Œç›®å½•åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œæˆ–è€…æ˜¾å¼æŒ‡å®š .env è·¯å¾„
load_dotenv() 

class HelloAgentsLLM:
    """
    é€‚é… Datawhale Hello Agents æ•™ç¨‹çš„ LLM å®¢æˆ·ç«¯ã€‚
    æ”¯æŒè‡ªåŠ¨åŠ è½½ .env ä¸­çš„é€šç”¨é…ç½®ï¼Œä¹Ÿæ”¯æŒä¼ å…¥ç‰¹å®šå‚æ•°ã€‚
    """
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):
        # 1. å°è¯•ä½¿ç”¨ä¼ å…¥å‚æ•°
        # 2. å°è¯•è¯»å–æ•™ç¨‹æ ‡å‡†çš„é€šç”¨ç¯å¢ƒå˜é‡ (LLM_*)
        # 3. å…œåº•ï¼šå°è¯•è¯»å–é¡¹ç›®ä¸­å·²æœ‰çš„ç‰¹å®šå‚å•†ç¯å¢ƒå˜é‡ (å¦‚ QWEN_*) ä»¥æ–¹ä¾¿ç›´æ¥ä½¿ç”¨
        
        self.model = model or os.getenv("LLM_MODEL_ID") or "Mimo-V2-flash" # é»˜è®¤ç¤ºä¾‹
        
        # è‡ªåŠ¨é€‚é… Key å’Œ BaseURL
        self.api_key = apiKey or os.getenv("LLM_API_KEY")
        self.base_url = baseUrl or os.getenv("LLM_BASE_URL")
        self.timeout = timeout or int(os.getenv("LLM_TIMEOUT", 60))
        
        # å¦‚æœæ²¡æœ‰é€šç”¨çš„ LLM_API_KEYï¼Œå°è¯•è‡ªåŠ¨é€šè¿‡æ¨¡å‹ååŒ¹é…å·²æœ‰çš„ Key (å¯é€‰ä¼˜åŒ–)
        if not self.api_key:
            if "qwen" in self.model.lower():
                self.api_key = os.getenv("QWEN_API_KEY")
                self.base_url = os.getenv("QWEN_BASE_URL")
            elif "deepseek" in self.model.lower():
                self.api_key = os.getenv("DEEPSEEK_API_KEY")
                self.base_url = os.getenv("DEEPSEEK_BASE_URL")
            # ... å…¶ä»–æ¨¡å‹é€‚é…
            
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° API Keyã€‚è¯·åœ¨ .env ä¸­é…ç½® LLM_API_KEY æˆ–ç‰¹å®šçš„æ¨¡å‹ Keyã€‚")

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout
        )

    def think(self, messages: List[Dict[str, str]], temperature: float = 0.7, stream: bool = False) -> str:
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šå‘é€æ¶ˆæ¯å†å²å¹¶è·å–å›å¤
        :param stream: æ˜¯å¦å¼€å¯æµå¼è¾“å‡º (æ‰“å°åˆ°æ§åˆ¶å°)
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=stream 
            )
            
            if not stream:
                # éæµå¼ï¼šç›´æ¥è¿”å›
                print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ!", flush=True)
                return response.choices[0].message.content
            else:
                # æµå¼å¤„ç†é€»è¾‘
                collected_content = []
                print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ!", flush=True)
                
                for chunk in response:
                    # 1. å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœ choices ä¸ºç©ºï¼Œç›´æ¥è·³è¿‡
                    if not chunk.choices:
                        continue
                    # 2. è·å–å†…å®¹ï¼šå¦‚æœ delta ä¸ºç©ºï¼Œç›´æ¥è·³è¿‡
                    content = chunk.choices[0].delta.content
                    # 3. å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœ content æ˜¯ Noneï¼ˆæœ‰æ—¶å¯èƒ½æ˜¯ç©ºå­—ç¬¦ä¸²æˆ– Noneï¼‰ï¼Œä¹Ÿè·³è¿‡
                    if content:
                        print(content, end="", flush=True)
                        collected_content.append(content)
                print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ
                return "".join(collected_content)

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return f"Error calling LLM: {str(e)}"

# --- å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == '__main__':
    try:
        llmClient = HelloAgentsLLM()
        
        exampleMessages = [
            {"role": "system", "content": "You are a helpful assistant that writes Python code."},
            {"role": "user", "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"}
        ]
        
        print("--- è°ƒç”¨LLM ---")
        responseText = llmClient.think(exampleMessages)
        if responseText:
            print("\n\n--- å®Œæ•´æ¨¡å‹å“åº” ---")
            print(responseText)

    except ValueError as e:
        print(e)